{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "\n",
    "<img src=\"images/anaconda_logo.png\" style=\"float: none; margin: 10px 10px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://www.anaconda.com/download/\n",
    "\n",
    "**What is it?** Anaconda is a free and open-source distribution of the Python that aims to simplify package management and deployment. It includes the most commonly used libraries, as well as pip, the Python package manager used to install and manage new libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**\n",
    "\n",
    "<img src=\"images/image_01a.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "<img src=\"images/image_01b.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "<img src=\"images/image_01c.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/git_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 100px;\"/>\n",
    "\n",
    "**Navigate to:** https://git-scm.com/download\n",
    "\n",
    "**What is it?** Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_02.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/github_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://github.com/\n",
    "\n",
    "**What is it?**  Github is a web-based hosting service for version control using Git. It offers all of the distributed version control and source code management functionality of Git as well as access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_03.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/aws_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://aws.amazon.com/free/\n",
    "\n",
    "**What is it?** Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms to individuals, companies and governments, on a paid subscription basis. The technology allows subscribers to have at their disposal a virtual cluster of computers, available all the time, through the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_04.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gcp_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 250px;\"/>\n",
    "\n",
    "**Navigate to:** https://cloud.google.com/\n",
    "\n",
    "**What is it?** Google Cloud Platform (GCP) is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search and YouTube. Alongside a set of management tools, it provides a series of modular cloud services including computing, data storage, data analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_05.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "Using the command line, navigate to the directory where you want to store this training:\n",
    "\n",
    "`cd`\n",
    "\n",
    "After you have copied the HTTPS clone link, run the command:\n",
    "\n",
    "`git clone https://github.com/araveret/training.git`\n",
    "\n",
    "Navigate into your new repository and install the required packages with the command: \n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Base Python\n",
    "Using the command line, open Spyder with the command: \n",
    "\n",
    "`spyder`\n",
    "\n",
    "Once Spyder opens, drag the **01_base_python.py** file into the text editor on the left side of the integrated developer environment (IDE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Collection\n",
    "## Flat file\n",
    "**Use Case:** Titanic (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the titanic data in the data folder\n",
    "import pandas as pd\n",
    "path = r'./data/titanic.csv'\n",
    "titanic = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.columns = ['survived', 'pclass', 'name', 'sex', 'age', 'sib_spouse', 'parent_child', 'fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "**Use Case:** Titanic (AWS RDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "\n",
    "# create functions to gather all the information you need in a create statement\n",
    "def get_length(df, column):\n",
    "    return str(max([len(str(x)) for x in df[column]]))\n",
    "\n",
    "def get_type(df, column):\n",
    "    return key[str(df[column].dtypes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the name of the table and a key for columns types\n",
    "table = 'titanic'\n",
    "key = {'object':'varchar',\n",
    "       'int64':'int',\n",
    "       'float64':'float',\n",
    "       'datetime64[ns]':'date',\n",
    "       'bool':'boolean'\n",
    "       }\n",
    "\n",
    "# build your create statement\n",
    "drop = \"DROP TABLE IF EXISTS \"+table+\" CASCADE;\"\n",
    "create = \"CREATE TABLE \"+table+\"(\"+ \\\n",
    "        ', '.join([column +' '+ get_type(titanic, column) +'({})'.format(get_length(titanic, column)) \\\n",
    "        if (get_type(titanic, column)!='date')&(get_type(titanic, column)!='int') \\\n",
    "        else column +' '+ get_type(titanic, column) \\\n",
    "        for column in titanic.columns]) +');'\n",
    "statement = ' '.join([drop, create])\n",
    "\n",
    "# initialize a string buffer\n",
    "sio = StringIO()\n",
    "sio.write(titanic.to_csv(index=None, header=None))\n",
    "sio.seek(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the string buffer that the last step created\n",
    "sio.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a table and copy data into it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection with the database\n",
    "conn = psycopg2.connect(' '.join(['host=HOST_HERE',\n",
    "                                  'dbname=DBNAME_HERE',\n",
    "                                  'user=USER_HERE',\n",
    "                                  'password=PASSWORD_HERE']))\n",
    "\n",
    "# create a table to store your data\n",
    "with conn.cursor() as c:\n",
    "    c.execute(statement)\n",
    "    conn.commit()\n",
    "\n",
    "# copy the string buffer to the table\n",
    "with conn.cursor() as c:\n",
    "    c.copy_from(sio, \"titanic\", columns=titanic.columns, sep=',')\n",
    "    conn.commit()\n",
    "\n",
    "# close the connection with the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute a query and store results as a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection with the database\n",
    "\n",
    "conn = psycopg2.connect(' '.join(['host=HOST_HERE',\n",
    "                                  'dbname=DBNAME_HERE',\n",
    "                                  'user=USER_HERE',\n",
    "                                  'password=PASSWORD_HERE']))\n",
    "\n",
    "# fetch and store all data from the table\n",
    "with conn.cursor() as c:\n",
    "    c.execute('SELECT * FROM '+table)\n",
    "    data = c.fetchall()\n",
    "    \n",
    "# close the conenction with the database\n",
    "conn.close()\n",
    "\n",
    "# convert stored data into a Pandas dataframe\n",
    "df = pd.DataFrame(data, columns=titanic.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above code packaged into a Class** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class connection():\n",
    "    def __init__(self, host, dbname, user, password):\n",
    "        self.host = host\n",
    "        self.dbname = dbname\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = psycopg2.connect(' '.join(['host='+str(host),\n",
    "                                  'dbname='+str(dbname),\n",
    "                                  'user='+str(user),\n",
    "                                  'password='+str(password)]))\n",
    "        \n",
    "    def create_table(self, table, data):\n",
    "        self.data = data\n",
    "        self.table = table\n",
    "        self.key = {'object':'varchar',\n",
    "           'int64':'int',\n",
    "           'float64':'float',\n",
    "           'datetime64[ns]':'date',\n",
    "           'bool':'boolean'\n",
    "           }\n",
    "        \n",
    "        def get_length(df, column):\n",
    "            return str(max([len(str(x)) for x in df[column]]))\n",
    "\n",
    "        def get_type(df, column):\n",
    "            return self.key[str(df[column].dtypes)]\n",
    "        \n",
    "        self.drop = \"DROP TABLE IF EXISTS \"+self.table+\" CASCADE;\"\n",
    "        self.create = \"CREATE TABLE \"+self.table+\"(\"+ \\\n",
    "                ', '.join([column +' '+ get_type(self.data, column) +'({})'.format(get_length(self.data, column)) \\\n",
    "                if (get_type(self.data, column)!='date')&(get_type(self.data, column)!='int') \\\n",
    "                else column +' '+ get_type(self.data, column) \\\n",
    "                for column in self.data.columns]) +');'\n",
    "        self.statement = ' '.join([self.drop, self.create])\n",
    "        with self.conn.cursor() as c:\n",
    "            c.execute(self.statement)\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def fill_table(self, table, data):\n",
    "        self.table = table\n",
    "        self.data = data\n",
    "        self.sio = StringIO()\n",
    "        self.sio.write(self.data.to_csv(index=None, header=None))\n",
    "        self.sio.seek(0)\n",
    "        with self.conn.cursor() as c:\n",
    "            c.copy_from(self.sio, self.table, columns=self.data.columns, sep=',')\n",
    "            self.conn.commit()\n",
    "    \n",
    "    def query_table(self, query, columns):\n",
    "        self.columns = columns\n",
    "        self.query = query\n",
    "        with self.conn.cursor() as c:\n",
    "            c.execute(self.query)\n",
    "            self.data = c.fetchall()\n",
    "        self.df = pd.DataFrame(self.data, columns=self.columns)\n",
    "    \n",
    "    def close_connection(self):\n",
    "        self.conn.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database\n",
    "postgres = connection(host='HOST_HERE', \n",
    "                      dbname='DBNAME_HERE', \n",
    "                      user='USER_HERE', \n",
    "                      password='PASSWORD_HERE')\n",
    "\n",
    "# create the 'titanic' table\n",
    "postgres.create_table('titanic', titanic)\n",
    "\n",
    "# fill the 'titanic' table with the titanic dataframe\n",
    "postgres.fill_table('titanic', titanic)\n",
    "\n",
    "# query the 'titanic' table and store the data as a dataframe\n",
    "df = postgres.query_table('titanic', titanic.colums)\n",
    "\n",
    "# see the top records from the new titanic dataframe\n",
    "postgres.df\n",
    "\n",
    "# close the connection\n",
    "postgres.close_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping \n",
    "**Use Case:** Python.org Events (HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# request the html code from a url\n",
    "url = r'https://www.python.org/events/python-events/'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert HTML into a structured Soup object\n",
    "b = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text for one element in the table\n",
    "b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all of the elements by column in a dictionary\n",
    "dct = {'event':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('h3')],\n",
    "       'date':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('time')],\n",
    "       'location':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('span', attrs={'class':'event-location'})]\n",
    "      }\n",
    "\n",
    "# convert the dictionary into a data table\n",
    "events = pd.DataFrame(dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Repeat the above process to create a dataframe containing information about recent polls\n",
    "\n",
    "**Extra**: Create a function called \"GetPolls\" only for polls a specific date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.realclearpolitics.com/epolls/latest_polls/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "**Use Case:** Google Maps (REST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# part 1 - base path of the api\n",
    "main = r'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# part 2 - the address search component\n",
    "search = '555 Hamilton Avenue, Palo Alto, CA'\n",
    "\n",
    "# part 3 - your api developer key to authenticate the search\n",
    "api_key = r'&key=KEY_HERE'\n",
    "\n",
    "# put parts 1, 2, and 3 together\n",
    "url = main + urllib.parse.urlencode({'address':search})+api_key\n",
    "\n",
    "# make a get request to the api\n",
    "result = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Python client \n",
    "import googlemaps\n",
    "\n",
    "gmaps = googlemaps.Client(key=r'KEY_HERE')\n",
    "search = ('555 Hamilton Avenue, Palo Alto, CA')\n",
    "result = gmaps.geocode(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your own googlemaps package\n",
    "class dumbgooglemaps():\n",
    "    def __init__(self):\n",
    "        import urllib.parse\n",
    "        import requests\n",
    "        import json\n",
    "        self.main = r'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "    def Client(self, key):\n",
    "        self.key = r'&key='+key\n",
    "    def geocode(self, search):\n",
    "        self.search = search\n",
    "        self.url = self.main + urllib.parse.urlencode({'address':self.search}) + self.api_key\n",
    "        return requests.get(self.url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your own googlemaps package\n",
    "dgmaps = dumbgooglemaps()\n",
    "dgmaps.Client(key='KEY_HERE')\n",
    "result = dgmaps.geocode(search=r'555 Hamilton Avenue, Palo Alto, CA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the Star Wars API to find the name of the \"homeworld\" of \"Han Solo\"\n",
    "\n",
    "**Extra**: Create a class called 'StarWarsAPI' with a function called 'CharacterSearch', which returns the JSON data for any character you search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'http://swapi.co/api/\n",
    "'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ETL\n",
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for each passenger's title\n",
    "titanic['title'] = [x.split('.')[0] for x in titanic['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to group each passenger by age range\n",
    "age_group = []\n",
    "\n",
    "for age in titanic['age']:\n",
    "    if age < 1:\n",
    "        age_group.append('00 years')\n",
    "    elif age <= 4:\n",
    "        age_group.append('01-04 years')\n",
    "    elif age <= 9:\n",
    "        age_group.append('05-09 years')\n",
    "    elif age <= 14:\n",
    "        age_group.append('10-14 years')\n",
    "    elif age <= 19:\n",
    "        age_group.append('15-19 years')\n",
    "    elif age <= 24:\n",
    "        age_group.append('20-24 years')\n",
    "    elif age <= 29:\n",
    "        age_group.append('25-29 years')\n",
    "    elif age <= 34:\n",
    "        age_group.append('30-34 years')\n",
    "    elif age <= 39:\n",
    "        age_group.append('35-39 years')\n",
    "    elif age <= 44:\n",
    "        age_group.append('40-44 years')\n",
    "    elif age <= 49:\n",
    "        age_group.append('45-49 years')\n",
    "    elif age <= 54:\n",
    "        age_group.append('50-54 years')\n",
    "    else:\n",
    "        age_group.append('55+ years')\n",
    "        \n",
    "titanic['age_group'] = age_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow\n",
    "\n",
    "To install Aiflow on your local machine, enter in your terminal:\n",
    "\n",
    "`pip install apache-airflow`\n",
    "\n",
    "Once Airflow has installed, you will need to initiate a database which stores all of the information Airflow needs to run:\n",
    "\n",
    "`airflow initdb`\n",
    "\n",
    "Replace your current 'dags' directory, `Users/[YOUR_USERNAME]/airflow/dags/`, with the directory inside your training repository.\n",
    "\n",
    "Create a 'stage' directory in airflow, beside the 'dags' folder, to store stage data: `Users/[YOUR_USERNAME]/airflow/stage`\n",
    "\n",
    "Next, run the Airflow scheduler in your terminal:\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
    "Then, in a new terminal window, run the Airflow webserver:\n",
    "\n",
    "`airflow webserver -p 8080`\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "*Step 1* - In a browswer, navigate to `http://localhost:8080/admin/` to interact with your Airflow UI.\n",
    "\n",
    "*Step 2* - Review the dag 'titanic_example.py' as well as the package 'titanic_main.py' (in the 'library' folder) to better understand how to set up a dag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - When you are finished with Airflow you can close the scheduler and kill the webserver with the following command:\n",
    "\n",
    "`cat airflow/airflow-webserver.pid | xargs kill -9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Statistical Modeling\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_1.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_2.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass = pd.get_dummies(titanic['pclass'], prefix='pclass').iloc[:,:-1]\n",
    "sex = pd.get_dummies(titanic['sex'], prefix='sex').iloc[:,:-1]\n",
    "title = pd.get_dummies(titanic['title'], prefix='title').iloc[:,:-1]\n",
    "age_group = pd.get_dummies(titanic['age_group'], prefix='age_group').iloc[:,:-1]\n",
    "X = pd.concat([pclass, sex, title, age_group], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(random_state=1)\n",
    "depth_range = list(range(1,20))\n",
    "param_grid = dict(max_depth=depth_range)\n",
    "grid = GridSearchCV(dtree, param_grid, cv=5, scoring='roc_auc')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_scores = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(depth_range, grid_mean_scores)\n",
    "plt.grid(True)\n",
    "plt.plot(grid.best_params_['max_depth'], grid.best_score_, 'ro', markersize=12, markeredgewidth=1.5, markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(best.feature_importances_, index=X.columns.tolist())[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = best.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('FPR (1 - Specificity)')\n",
    "plt.ylabel('TPR (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the above model to a Random Forest Classifier tuning the 'n_estimators' and 'max_depth' parameters\n",
    "from sklearn import ensemble\n",
    "\n",
    "rforest = ensemble.RandomForestClassifier(random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Deployment\n",
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = r'./models/dtree_model.pkl'\n",
    "pickle.dump(best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "metrics.accuracy_score(y, loaded_model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Web Application\n",
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
