{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science 101\n",
    "by Adam Raveret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "\n",
    "<img src=\"images/anaconda_logo.png\" style=\"float: none; margin: 10px 10px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://www.anaconda.com/download/\n",
    "\n",
    "**What is it?** Anaconda is a free and open-source distribution of the Python that aims to simplify package management and deployment. It includes the most commonly used libraries, as well as pip, the Python package manager used to install and manage new libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**\n",
    "\n",
    "<img src=\"images/image_01a.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "<img src=\"images/image_01b.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "<img src=\"images/image_01c.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/git_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 100px;\"/>\n",
    "\n",
    "**Navigate to:** https://git-scm.com/download\n",
    "\n",
    "**What is it?** Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_02.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/github_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://github.com/\n",
    "\n",
    "**What is it?**  Github is a web-based hosting service for version control using Git. It offers all of the distributed version control and source code management functionality of Git as well as access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_03.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/aws_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 200px;\"/>\n",
    "\n",
    "**Navigate to:** https://aws.amazon.com/free/\n",
    "\n",
    "**What is it?** Amazon Web Services (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms to individuals, companies and governments, on a paid subscription basis. The technology allows subscribers to have at their disposal a virtual cluster of computers, available all the time, through the Internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_04.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gcp_logo.png\" style=\"float: none; margin: 15px 15px 0 0; width: 250px;\"/>\n",
    "\n",
    "**Navigate to:** https://cloud.google.com/\n",
    "\n",
    "**What is it?** Google Cloud Platform (GCP) is a suite of cloud computing services that runs on the same infrastructure that Google uses internally for its end-user products, such as Google Search and YouTube. Alongside a set of management tools, it provides a series of modular cloud services including computing, data storage, data analytics and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/image_05.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages\n",
    "Using the command line, navigate to the directory where you want to store this training:\n",
    "\n",
    "`cd`\n",
    "\n",
    "After you have copied the HTTPS clone link, run the command:\n",
    "\n",
    "`git clone https://github.com/araveret/training.git`\n",
    "\n",
    "Navigate into your new repository and install the required packages with the command: \n",
    "\n",
    "`pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Base Python\n",
    "Using the command line, open Spyder with the command: \n",
    "\n",
    "`spyder`\n",
    "\n",
    "Once Spyder opens, drag the 'base_python.py' file into the text editor on the left side of the integrated developer environment (IDE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Collection\n",
    "## Flat file\n",
    "**Use Case:** Titanic (CSV)\n",
    "\n",
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the titanic data in the data folder\n",
    "path = r'./data/titanic.csv'\n",
    "titanic = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.columns = ['survived', 'pclass', 'name', 'sex', 'age', 'sib_spouse', 'parent_child', 'fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "**Use Case:** Titanic (AWS RDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "\n",
    "# create a function to create a dictionary which contains all the information you need in a create statement\n",
    "def create_dict(df):\n",
    "    dct = {}\n",
    "    for n,i in enumerate(df.dtypes):\n",
    "        dct[df.columns[n]] = [str(i),str(max([len(x) for x in df[df.columns[n]].astype(str)]))]\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary, the name of the table, and a key for columns types\n",
    "dct = create_dict(titanic)\n",
    "table = 'titanic'\n",
    "key = {'object':'varchar',\n",
    "       'int64':'int',\n",
    "       'float64':'float',\n",
    "       'datetime64[ns]':'date',\n",
    "       'bool':'boolean'\n",
    "       }\n",
    "\n",
    "# build your create statement\n",
    "create = \"DROP TABLE IF EXISTS \"+table+\" CASCADE; CREATE TABLE \"+table+\"(\"+ \\\n",
    "    ', '.join([item+' '+key[dct[item][0]]+'('+dct[item][1]+')' \n",
    "               if ((key[dct[item][0]]!='date')&(key[dct[item][0]]!='int')) \n",
    "               else item+' '+key[dct[item][0]] for item in titanic.columns]) \\\n",
    "    +');'\n",
    "\n",
    "# initialize a string buffer\n",
    "sio = StringIO()\n",
    "sio.write(titanic.to_csv(index=None, header=None))\n",
    "sio.seek(0)\n",
    "#sio.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection with the database\n",
    "conn = psycopg2.connect(' '.join(['host=PUT_HOST_HERE',\n",
    "                                  'dbname=PUT_DBNAME_HERE',\n",
    "                                  'user=PUT_USER_HERE',\n",
    "                                  'password=PUT_PASS_HERE']))\n",
    "\n",
    "# create a table to store your data\n",
    "with conn.cursor() as c:\n",
    "    c.execute(create)\n",
    "    conn.commit()\n",
    "\n",
    "# copy the string buffer to the table\n",
    "with conn.cursor() as c:\n",
    "    c.copy_from(sio, \"titanic\", columns=titanic.columns, sep=',')\n",
    "    conn.commit()\n",
    "\n",
    "# close the connection with the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a connection with the database\n",
    "conn = psycopg2.connect(' '.join(['host=PUT_HOST_HERE',\n",
    "                                  'dbname=PUT_DBNAME_HERE',\n",
    "                                  'user=PUT_USER_HERE',\n",
    "                                  'password=PUT_PASS_HERE']))\n",
    "\n",
    "# fetch and store all data from the table\n",
    "with conn.cursor() as c:\n",
    "    c.execute('SELECT * FROM '+table)\n",
    "    data = c.fetchall()\n",
    "    \n",
    "# close the conenction with the database\n",
    "conn.close()\n",
    "\n",
    "# convert stored data into a Pandas dataframe\n",
    "df2 = pd.DataFrame(data, columns=titanic.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping \n",
    "**Use Case:** Python.org Events (HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# request the html code from a url\n",
    "url = r'https://www.python.org/events/python-events/'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert HTML into a structured Soup object\n",
    "b = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the text for one element in the table\n",
    "b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all of the elements by column in a dictionary\n",
    "dct = {'event':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('h3')],\n",
    "       'date':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('time')],\n",
    "       'location':[x.text for x in b.find_all('ul', attrs={'class':\"list-recent-events menu\"})[0].find_all('span', attrs={'class':'event-location'})]\n",
    "      }\n",
    "\n",
    "# convert the dictionary into a data table\n",
    "events = pd.DataFrame(dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above process to create a dataframe containing information about recent polls\n",
    "url = r'https://www.realclearpolitics.com/epolls/latest_polls/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "**Use Case:** Google Maps (REST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# part 1 - base path of the api\n",
    "main = r'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "\n",
    "# part 2 - the address search component\n",
    "search = '555 Hamilton Avenue, Palo Alto, CA'\n",
    "\n",
    "# part 3 - your api developer key to authenticate the search\n",
    "api_key = r'&key=API_KEY_HERE'\n",
    "\n",
    "# put parts 1, 2, and 3 together\n",
    "url = main + urllib.parse.urlencode({'address':search})+api_key\n",
    "\n",
    "# make a get request to the api\n",
    "map_data = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the Python client \n",
    "import googlemaps\n",
    "\n",
    "gmaps = googlemaps.Client(key=r'API_KEY_HERE')\n",
    "search = ('555 Hamilton Avenue, Palo Alto, CA')\n",
    "map_data = gmaps.geocode(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the Star Wars API to find the name of the \"homeworld\" of \"Han Solo\"\n",
    "url = r'http://swapi.co/api/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ETL\n",
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for each passenger's title\n",
    "titanic['title'] = [x.split('.')[0] for x in titanic['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to group each passenger by age range\n",
    "age_group = []\n",
    "\n",
    "for age in titanic['age']:\n",
    "    if age < 1:\n",
    "        age_group.append('00 years')\n",
    "    elif age <= 4:\n",
    "        age_group.append('01-04 years')\n",
    "    elif age <= 9:\n",
    "        age_group.append('05-09 years')\n",
    "    elif age <= 14:\n",
    "        age_group.append('10-14 years')\n",
    "    elif age <= 19:\n",
    "        age_group.append('15-19 years')\n",
    "    elif age <= 24:\n",
    "        age_group.append('20-24 years')\n",
    "    elif age <= 29:\n",
    "        age_group.append('25-29 years')\n",
    "    elif age <= 34:\n",
    "        age_group.append('30-34 years')\n",
    "    elif age <= 39:\n",
    "        age_group.append('35-39 years')\n",
    "    elif age <= 44:\n",
    "        age_group.append('40-44 years')\n",
    "    elif age <= 49:\n",
    "        age_group.append('45-49 years')\n",
    "    elif age <= 54:\n",
    "        age_group.append('50-54 years')\n",
    "    else:\n",
    "        age_group.append('55+ years')\n",
    "        \n",
    "titanic['age_group'] = age_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow\n",
    "\n",
    "To install Aiflow on your local machine, enter in your terminal:\n",
    "\n",
    "`pip install apache-airflow`\n",
    "\n",
    "Once Airflow has installed, you will need to initiate a database which stores all of the information Airflow needs to run:\n",
    "\n",
    "`airflow initdb`\n",
    "\n",
    "Store the 'titanic_example.py' file in your Airflow directory:\n",
    "\n",
    "`Users/[YOUR_USERNAME]/airflow/dags/titanic_example.py`\n",
    "\n",
    "Create a 'stage' directory beside the 'dags' folder to store stage data:\n",
    "\n",
    "`Users/[YOUR_USERNAME]/airflow/stage`\n",
    "\n",
    "Next, run the Airflow scheduler in your terminal:\n",
    "\n",
    "`airflow scheduler`\n",
    "\n",
    "Then, in a new terminal window, run the Airflow webserver:\n",
    "\n",
    "`airflow webserver -p 8080`\n",
    "\n",
    "When you are finished with Airflow you can close the scheduler and kill the webserver with the following command:\n",
    "\n",
    "`cat airflow/airflow-webserver.pid | xargs kill -9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Statistical Modeling\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Linear regression is a linear approach to modelling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_1.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/regression_2.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is it?**\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you do it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you evaluate it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass = pd.get_dummies(titanic['pclass'], prefix='pclass').iloc[:,:-1]\n",
    "sex = pd.get_dummies(titanic['sex'], prefix='sex').iloc[:,:-1]\n",
    "title = pd.get_dummies(titanic['title'], prefix='title').iloc[:,:-1]\n",
    "age_group = pd.get_dummies(titanic['age_group'], prefix='age_group').iloc[:,:-1]\n",
    "X = pd.concat([pclass, sex, title, age_group], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeClassifier(random_state=1)\n",
    "depth_range = list(range(1,20))\n",
    "param_grid = dict(max_depth=depth_range)\n",
    "grid = GridSearchCV(dtree, param_grid, cv=5, scoring='roc_auc')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mean_scores = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(depth_range, grid_mean_scores)\n",
    "plt.grid(True)\n",
    "plt.plot(grid.best_params_['max_depth'], grid.best_score_, 'ro', markersize=12, markeredgewidth=1.5, markerfacecolor='None', markeredgecolor='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(best.feature_importances_, index=X.columns.tolist())[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = best.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('FPR (1 - Specificity)')\n",
    "plt.ylabel('TPR (Sensitivity)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the above model to a Random Forest Classifier tuning the 'n_estimators' and 'max_depth' parameters\n",
    "rforest = ensemble.RandomForestClassifier(random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Deployment\n",
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = r'./model/dtree_model.pkl'\n",
    "pickle.dump(best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "metrics.accuracy_score(y, loaded_model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Web Application\n",
    "## Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
